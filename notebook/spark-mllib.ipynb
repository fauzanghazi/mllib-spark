{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "027201e0",
   "metadata": {},
   "source": [
    "# Machine Learning with Apache MLlib via PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e3087c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "This notebook demonstrates the use of **Apache MLlib**, the scalable machine learning library built on **Apache Spark**, to perform end-to-end regression modeling. Using **PySpark**, we process the California housing dataset, apply data cleaning, feature engineering, and train a linear regression model.\n",
    "\n",
    "The workflow includes Spark-native techniques like `Imputer`, `VectorAssembler`, and `StandardScaler` for pipeline construction, ensuring scalability across distributed environments.\n",
    "\n",
    "Finally, we evaluate the model using `RegressionMetrics` to assess performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cf4724",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Install PySpark\n",
    "\n",
    "This installs the PySpark library, which provides the Python API for Apache Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fef77066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Using cached pyspark-4.0.0.tar.gz (434.1 MB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting py4j==0.10.9.9 (from pyspark)\n",
      "  Using cached py4j-0.10.9.9-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Using cached py4j-0.10.9.9-py2.py3-none-any.whl (203 kB)\n",
      "Building wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py): started\n",
      "  Building wheel for pyspark (setup.py): still running...\n",
      "  Building wheel for pyspark (setup.py): finished with status 'done'\n",
      "  Created wheel for pyspark: filename=pyspark-4.0.0-py2.py3-none-any.whl size=434741340 sha256=98867c7e53a093aff70487b0dd40dd0206329b5937f4e5e6c228ac1992d2f9f5\n",
      "  Stored in directory: c:\\users\\jihit\\appdata\\local\\pip\\cache\\wheels\\2d\\77\\9b\\12660be70f7f447940a0caede37ae208b2e0d1c8487dce52a6\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.9 pyspark-4.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa04c75",
   "metadata": {},
   "source": [
    "### Initialize Spark Session\n",
    "\n",
    "Creates a local Spark session named **MRTB1163** with a custom UI port (4050).\n",
    "\n",
    "This session allows Spark operations within VSCode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af477d74",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.lang.UnsupportedOperationException: getSubject is not supported\r\n\tat java.base/javax.security.auth.Subject.getSubject(Subject.java:277)\r\n\tat org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:588)\r\n\tat org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2446)\r\n\tat scala.Option.getOrElse(Option.scala:201)\r\n\tat org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2446)\r\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:339)\r\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59)\r\n\tat java.base/jdk.internal.reflect.DirectConstructorHandleAccessor.newInstance(DirectConstructorHandleAccessor.java:62)\r\n\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:499)\r\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:483)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:238)\r\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\r\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1447)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 7\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[0;32m      3\u001b[0m spark \u001b[38;5;241m=\u001b[39m \u001b[43mSparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaster\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlocal\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMRTB1163\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mspark.ui.port\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m4050\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m----> 7\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m spark\n",
      "File \u001b[1;32mc:\\Users\\jihit\\.conda\\envs\\test-env\\Lib\\site-packages\\pyspark\\sql\\session.py:556\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    554\u001b[0m     sparkConf\u001b[38;5;241m.\u001b[39mset(key, value)\n\u001b[0;32m    555\u001b[0m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[1;32m--> 556\u001b[0m sc \u001b[38;5;241m=\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    557\u001b[0m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[0;32m    558\u001b[0m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[0;32m    559\u001b[0m session \u001b[38;5;241m=\u001b[39m SparkSession(sc, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options)\n",
      "File \u001b[1;32mc:\\Users\\jihit\\.conda\\envs\\test-env\\Lib\\site-packages\\pyspark\\core\\context.py:523\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[1;34m(cls, conf)\u001b[0m\n\u001b[0;32m    521\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    522\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 523\u001b[0m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    524\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    525\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n",
      "File \u001b[1;32mc:\\Users\\jihit\\.conda\\envs\\test-env\\Lib\\site-packages\\pyspark\\core\\context.py:207\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[0;32m    205\u001b[0m SparkContext\u001b[38;5;241m.\u001b[39m_ensure_initialized(\u001b[38;5;28mself\u001b[39m, gateway\u001b[38;5;241m=\u001b[39mgateway, conf\u001b[38;5;241m=\u001b[39mconf)\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 207\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_init\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    208\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaster\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    209\u001b[0m \u001b[43m        \u001b[49m\u001b[43mappName\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    210\u001b[0m \u001b[43m        \u001b[49m\u001b[43msparkHome\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    211\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpyFiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    212\u001b[0m \u001b[43m        \u001b[49m\u001b[43menvironment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    213\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatchSize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserializer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    216\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjsc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    217\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprofiler_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[43mudf_profiler_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    219\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmemory_profiler_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    220\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;66;03m# If an error occurs, clean up in order to allow future SparkContext creation:\u001b[39;00m\n\u001b[0;32m    223\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop()\n",
      "File \u001b[1;32mc:\\Users\\jihit\\.conda\\envs\\test-env\\Lib\\site-packages\\pyspark\\core\\context.py:300\u001b[0m, in \u001b[0;36mSparkContext._do_init\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[0;32m    297\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvironment[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPYTHONHASHSEED\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPYTHONHASHSEED\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    299\u001b[0m \u001b[38;5;66;03m# Create the Java SparkContext through Py4J\u001b[39;00m\n\u001b[1;32m--> 300\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsc \u001b[38;5;241m=\u001b[39m jsc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initialize_context\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    301\u001b[0m \u001b[38;5;66;03m# Reset the SparkConf to the one actually used by the SparkContext in JVM.\u001b[39;00m\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conf \u001b[38;5;241m=\u001b[39m SparkConf(_jconf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsc\u001b[38;5;241m.\u001b[39msc()\u001b[38;5;241m.\u001b[39mconf())\n",
      "File \u001b[1;32mc:\\Users\\jihit\\.conda\\envs\\test-env\\Lib\\site-packages\\pyspark\\core\\context.py:429\u001b[0m, in \u001b[0;36mSparkContext._initialize_context\u001b[1;34m(self, jconf)\u001b[0m\n\u001b[0;32m    425\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    426\u001b[0m \u001b[38;5;124;03mInitialize SparkContext in function to allow subclass specific initialization\u001b[39;00m\n\u001b[0;32m    427\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    428\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 429\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mJavaSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjconf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jihit\\.conda\\envs\\test-env\\Lib\\site-packages\\py4j\\java_gateway.py:1627\u001b[0m, in \u001b[0;36mJavaClass.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1621\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCONSTRUCTOR_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1622\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_command_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1623\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1624\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1626\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1627\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1628\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fqn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1630\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1631\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\jihit\\.conda\\envs\\test-env\\Lib\\site-packages\\py4j\\protocol.py:327\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    325\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    326\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 327\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    329\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    330\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    331\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    333\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.lang.UnsupportedOperationException: getSubject is not supported\r\n\tat java.base/javax.security.auth.Subject.getSubject(Subject.java:277)\r\n\tat org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:588)\r\n\tat org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2446)\r\n\tat scala.Option.getOrElse(Option.scala:201)\r\n\tat org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2446)\r\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:339)\r\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59)\r\n\tat java.base/jdk.internal.reflect.DirectConstructorHandleAccessor.newInstance(DirectConstructorHandleAccessor.java:62)\r\n\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:499)\r\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:483)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:238)\r\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\r\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1447)\r\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "        .master(\"local\")\\\n",
    "        .appName(\"MRTB1163\")\\\n",
    "        .config('spark.ui.port', '4050')\\\n",
    "        .getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4a1552",
   "metadata": {},
   "source": [
    "### Load and Preview Dataset\n",
    "\n",
    "Reads the `housing.csv` file into a Spark DataFrame with headers and inferred data types.\n",
    "\n",
    "Displays the schema and shows the first 5 rows for a quick preview."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09ec3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\").load(\"housing.csv\", header=True, inferSchema=True)\n",
    "\n",
    "df.printSchema()\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee7feea",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Add ID Column and Basic Aggregation\n",
    "\n",
    "Adds a unique `id` column to each row using `monotonically_increasing_id()`.\n",
    "\n",
    "Reorders columns to place `id` first.  \n",
    "\n",
    "Displays a sample of 3 rows, counts total records, and computes the average of `total_rooms`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71448298",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "df = df.withColumn('id', monotonically_increasing_id())\n",
    "\n",
    "df = df[['id'] + df.columns[:-1]]\n",
    "\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef1282b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e5a509",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select('total_rooms').agg({'total_rooms': 'avg'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a570c2a8",
   "metadata": {},
   "source": [
    "### Summary Statistics and Grouped Aggregation\n",
    "\n",
    "Calculates the mean for all columns in the dataset.  \n",
    "\n",
    "Then, groups the data by `ocean_proximity` and computes the average for selected numerical columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d3c36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import mean\n",
    "\n",
    "df.select(*[mean(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802d8a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('ocean_proximity').agg({col: 'avg' for col in df.columns[3:-1]}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0185f49",
   "metadata": {},
   "source": [
    "### Custom UDF for Feature Transformation\n",
    "\n",
    "Defines a user-defined function (UDF) to square the `total_rooms` column.\n",
    "\n",
    "Applies the UDF to create a new column `total_rooms_squared` and displays the first 5 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94b792d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "def squared(value):\n",
    "  return value * value\n",
    "\n",
    "squared_udf = udf(squared, FloatType())\n",
    "\n",
    "df.withColumn('total_rooms_squared', squared_udf('total_rooms')).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d038a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d888a5fb",
   "metadata": {},
   "source": [
    "### Train-Test Split and Feature Selection\n",
    "\n",
    "Splits the dataset into training (70%) and testing (30%) sets.\n",
    "\n",
    "Removes non-numerical and label columns from the feature list to prepare for model input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35e6147",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = df.randomSplit([0.7, 0.3])\n",
    "\n",
    "train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12712689",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features_lst = train.columns\n",
    "numerical_features_lst.remove('median_house_value')\n",
    "numerical_features_lst.remove('id')\n",
    "numerical_features_lst.remove('ocean_proximity')\n",
    "\n",
    "numerical_features_lst"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01935707",
   "metadata": {},
   "source": [
    "### Handle Missing Values with Imputer\n",
    "\n",
    "Uses `Imputer` from `pyspark.ml` to fill missing values in selected numerical columns.  \n",
    "\n",
    "Applies the transformation to both training and testing datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc21558",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "imputer = Imputer(inputCols=numerical_features_lst,\n",
    "                  outputCols=numerical_features_lst)\n",
    "\n",
    "imputer = imputer.fit(train)\n",
    "\n",
    "train = imputer.transform(train)\n",
    "test = imputer.transform(test)\n",
    "\n",
    "train.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257e9a60",
   "metadata": {},
   "source": [
    "### Assemble Numerical Features\n",
    "\n",
    "Combines all selected numerical columns into a single vector column named `numerical_feature_vector`.\n",
    "\n",
    "Prepares the data for scaling and model input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f3e7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "numerical_vector_assembler = VectorAssembler(inputCols=numerical_features_lst,\n",
    "                                             outputCol='numerical_feature_vector')\n",
    "\n",
    "train = numerical_vector_assembler.transform(train)\n",
    "test = numerical_vector_assembler.transform(test)\n",
    "\n",
    "train.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51359035",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.select('numerical_feature_vector').take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d238b75",
   "metadata": {},
   "source": [
    "### Standardize Features\n",
    "\n",
    "Applies `StandardScaler` to normalize the numerical feature vector by removing the mean and scaling to unit variance.\n",
    "\n",
    "Outputs the result as `scaled_numerical_feature_vector` for both training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2431a6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "scaler = StandardScaler(inputCol='numerical_feature_vector',\n",
    "                        outputCol='scaled_numerical_feature_vector',\n",
    "                        withStd=True, withMean=True)\n",
    "\n",
    "scaler = scaler.fit(train)\n",
    "\n",
    "train = scaler.transform(train)\n",
    "test = scaler.transform(test)\n",
    "\n",
    "train.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7019599d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.select('scaled_numerical_feature_vector').take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20f7374",
   "metadata": {},
   "source": [
    "### Encode Categorical Feature\n",
    "\n",
    "Uses `StringIndexer` to convert the categorical `ocean_proximity` column into a numerical index named `ocean_category_index`.\n",
    "\n",
    "Applies the transformation to both training and testing datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774a15c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "indexer = StringIndexer(inputCol='ocean_proximity',\n",
    "                        outputCol='ocean_category_index')\n",
    "\n",
    "indexer = indexer.fit(train)\n",
    "train = indexer.transform(train)\n",
    "test = indexer.transform(test)\n",
    "\n",
    "train.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9328e8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Unique Encoded Categories\n",
    "\n",
    "set(train.select('ocean_category_index').collect()) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ac2186",
   "metadata": {},
   "source": [
    "### One-Hot Encode Categorical Feature\n",
    "\n",
    "Applies `OneHotEncoder` to convert the indexed categorical column into a sparse binary vector named `ocean_category_one_hot`.\n",
    "\n",
    "Used to prevent ordinal relationships in categorical features during model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2b24ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "one_hot_encoder = OneHotEncoder(inputCol='ocean_category_index',\n",
    "                                outputCol='ocean_category_one_hot')\n",
    "\n",
    "one_hot_encoder = one_hot_encoder.fit(train)\n",
    "\n",
    "train = one_hot_encoder.transform(train)\n",
    "test = one_hot_encoder.transform(test)\n",
    "\n",
    "train.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772380e2",
   "metadata": {},
   "source": [
    "### Combine All Features\n",
    "\n",
    "Merges scaled numerical features and one-hot encoded categorical features into a single column `final_feature_vector` for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0855236",
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=['scaled_numerical_feature_vector',\n",
    "                                       'ocean_category_one_hot'],\n",
    "                            outputCol='final_feature_vector')\n",
    "\n",
    "train = assembler.transform(train)\n",
    "test = assembler.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d801bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.select('final_feature_vector').take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74721c1",
   "metadata": {},
   "source": [
    "### Train Linear Regression Model and Predict\n",
    "\n",
    "1. Initialize a linear regression model with input features and target label.\n",
    "2. Fit the model on the training dataset.\n",
    "3. Apply the trained model to the training data to generate predictions and rename the prediction column for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b31c50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "lr = LinearRegression(featuresCol='final_feature_vector',\n",
    "                      labelCol='median_house_value')\n",
    "\n",
    "lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316320e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = lr.fit(train)\n",
    "\n",
    "lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba53ee7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train_df = lr.transform(train).withColumnRenamed('prediction',\n",
    "                                                      'predicted_median_house_value')\n",
    "\n",
    "pred_train_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052c67ec",
   "metadata": {},
   "source": [
    "### Predict on Test Data\n",
    "\n",
    "Applies the trained linear regression model to the test dataset and renames the prediction column to `predicted_median_house_value` for easier interpretation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de946e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test_df = lr.transform(test).withColumnRenamed('prediction', 'predicted_median_house_value')\n",
    "\n",
    "pred_test_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b554375",
   "metadata": {},
   "source": [
    "### Convert to Pandas DataFrame\n",
    "\n",
    "Converts the Spark DataFrame to a Pandas DataFrame for easier inspection or visualization using traditional Python libraries like matplotlib or seaborn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab84113c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test_pd_df = pred_test_df.toPandas()\n",
    "\n",
    "pred_test_pd_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42f8775",
   "metadata": {},
   "source": [
    "# Prepare Data for Regression Evaluation\n",
    "\n",
    "Extracts only the predicted and actual values for evaluation, then converts the Spark DataFrame to an RDD and maps the values to tuples, which is required for use with `RegressionMetrics` from MLlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c4b306",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_and_actuals = pred_test_df[['predicted_median_house_value',\n",
    "                                        'median_house_value']]\n",
    "                                    \n",
    "predictions_and_actuals_rdd = predictions_and_actuals.rdd\n",
    "\n",
    "predictions_and_actuals_rdd.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c6cbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_and_actuals_rdd = predictions_and_actuals_rdd.map(tuple)\n",
    "\n",
    "predictions_and_actuals_rdd.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cba8f3a",
   "metadata": {},
   "source": [
    "### Evaluate Model Performance\n",
    "\n",
    "Use `RegressionMetrics` from `pyspark.mllib.evaluation` to calculate and display evaluation metrics for the linear regression model such as:\n",
    "- Mean Squared Error (MSE)\n",
    "- Root Mean Squared Error (RMSE)\n",
    "- Mean Absolute Error (MAE)\n",
    "- and R-squared (R²)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6304a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "\n",
    "metrics = RegressionMetrics(predictions_and_actuals_rdd)\n",
    "\n",
    "s = '''\n",
    "Mean Squared Error:      {0}\n",
    "Root Mean Squared Error: {1}\n",
    "Mean Absolute Error:     {2}\n",
    "R**2:                    {3}\n",
    "'''.format(metrics.meanSquaredError,\n",
    "           metrics.rootMeanSquaredError,\n",
    "           metrics.meanAbsoluteError,\n",
    "           metrics.r2\n",
    "           )\n",
    "\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c6c301",
   "metadata": {},
   "source": [
    "### Visualize Actual vs Predicted (Plotly)\n",
    "\n",
    "This interactive scatter plot shows how close the model's predictions are to the actual values.\n",
    "\n",
    "The trendline offers a visual indicator of the model’s fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020e6acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fig = px.scatter(\n",
    "    pred_test_pd_df,\n",
    "    x='median_house_value',\n",
    "    y='predicted_median_house_value',\n",
    "    title='Actual vs Predicted House Values',\n",
    "    labels={'median_house_value': 'Actual', 'predicted_median_house_value': 'Predicted'},\n",
    "    opacity=0.7,\n",
    "    trendline='ols',\n",
    "    color='predicted_median_house_value'\n",
    ")\n",
    "\n",
    "fig.update_layout(showlegend=False)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9eacec3",
   "metadata": {},
   "source": [
    "### Plotly Bar Chart of Evaluation Metrics\n",
    "\n",
    "This chart provides a visual summary of the model’s error and accuracy metrics including R²."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c35147b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = go.Figure(data=[\n",
    "    go.Bar(name='Regression Metrics', x=['MSE', 'RMSE', 'MAE', 'R²'],\n",
    "           y=[metrics.meanSquaredError, metrics.rootMeanSquaredError, metrics.meanAbsoluteError, metrics.r2],\n",
    "           marker_color='indianred')\n",
    "])\n",
    "\n",
    "fig.update_layout(title='Model Evaluation Metrics',\n",
    "                  yaxis_title='Score',\n",
    "                  xaxis_title='Metric',\n",
    "                  template='plotly_white')\n",
    "fig.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
